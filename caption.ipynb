{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"colab":{"name":"caption.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"kdRFFuNTc61I","colab_type":"code","colab":{}},"source":["import torch \n","import torch.nn as nn\n","import torch.nn.functional as F \n","import numpy as np\n","import time\n","import cv2 \n","import argparse\n","import os \n","import os.path as osp\n","import pickle as pkl\n","import pandas as pd\n","import random\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt\n","import skimage\n","import pickle\n","#import bcolz\n","embedding_dim = 300\n","max_caption_length = 25\n","dim_image = 4096\n","dim_hidden1 = 1000\n","dim_hidden2 = 1000\n","dim_input1 = 1000\n","dim_input2 = 1300\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","yolo_weights_path = \"drive/My Drive/end_to_end/data/yolov3.weights\"#\"./data/yolov3.weights\"\n","coco_names_path =  \"drive/My Drive/end_to_end/data/coco.names\"       #\"./data/coco.names\"\n","yolo_config_path =  \"drive/My Drive/end_to_end/data/yolov3.cfg\"              #\"./data/yolov3.cfg\"\n","embedding_path =  \"drive/My Drive/end_to_end/data/embedding.npy\"  #\"./data/embedding.npy\"\n","wordtoindex_path =     \"drive/My Drive/end_to_end/data/wordtoindex.pickle\"                   #\"./data/wordtoindex.pickle\"\n","indextoword_path =    \"drive/My Drive/end_to_end/data/indextoword.pickle\"                  #\"./data/indextoword.pickle\"\n","video_path =    \"drive/My Drive/end_to_end/videos/dance.mp4\"    #\"./videos/0hyZ__3YhZc_485_490.avi\"\n","model_save_path = \"/content/drive/My Drive/main/teacher_model\"\n","model_num = 170"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"04roBGj7c61c","colab_type":"code","outputId":"c14bfd5b-b502-4337-b43e-8b17a80acf28","executionInfo":{"status":"ok","timestamp":1564340719538,"user_tz":-540,"elapsed":6178,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"loading embeddings and vocab.....\")\n","def save_dict(path , dct):\n","    with open(path , \"wb\") as handle:\n","        pickle.dump(dct , handle , protocol = pickle.HIGHEST_PROTOCOL)\n","def load_dict(path):\n","    with open(path, \"rb\") as handle:\n","        b = pickle.load(handle)\n","    return b\n","\n","embedding = torch.tensor(np.load(embedding_path)).to(device).double()\n","\n","wordtoindex = load_dict(wordtoindex_path)\n","\n","indextoword = load_dict(indextoword_path)\n","\n","\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["loading embeddings and vocab.....\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"P3CQ0iw_c61r","colab_type":"code","colab":{}},"source":["def parse_cfg(cfgfile):\n","    \"\"\"\n","    Takes a configuration file\n","    \n","    Returns a list of blocks. Each blocks describes a block in the neural\n","    network to be built. Block is represented as a dictionary in the list\n","    \n","    \"\"\"\n","    \n","    file = open(cfgfile, 'r')\n","    lines = file.read().split('\\n')                        # store the lines in a list\n","    lines = [x for x in lines if len(x) > 0]               # get read of the empty lines \n","    lines = [x for x in lines if x[0] != '#']              # get rid of comments\n","    lines = [x.rstrip().lstrip() for x in lines]           # get rid of fringe whitespaces\n","    \n","    block = {}\n","    blocks = []\n","    \n","    for line in lines:\n","        if line[0] == \"[\":               # This marks the start of a new block\n","            if len(block) != 0:          # If block is not empty, implies it is storing values of previous block.\n","                blocks.append(block)     # add it the blocks list\n","                block = {}               # re-init the block\n","            block[\"type\"] = line[1:-1].rstrip()     \n","        else:\n","            key,value = line.split(\"=\") \n","            block[key.rstrip()] = value.lstrip()\n","    blocks.append(block)\n","    \n","    return blocks"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"r_COakjoc610","colab_type":"code","colab":{}},"source":["def predict_transform(prediction, inp_dim, anchors, num_classes, CUDA = False):\n","\n","    \n","    batch_size = prediction.size(0)\n","    stride =  inp_dim // prediction.size(2)\n","    grid_size = inp_dim // stride\n","    bbox_attrs = 5 + num_classes\n","    num_anchors = len(anchors)\n","    \n","    prediction = prediction.view(batch_size, bbox_attrs*num_anchors, grid_size*grid_size)\n","    prediction = prediction.transpose(1,2).contiguous()\n","    prediction = prediction.view(batch_size, grid_size*grid_size*num_anchors, bbox_attrs)\n","    anchors = [(a[0]/stride, a[1]/stride) for a in anchors]\n","\n","    #Sigmoid the  centre_X, centre_Y. and object confidencce\n","    prediction[:,:,0] = torch.sigmoid(prediction[:,:,0])\n","    prediction[:,:,1] = torch.sigmoid(prediction[:,:,1])\n","    prediction[:,:,4] = torch.sigmoid(prediction[:,:,4])\n","    \n","    #Add the center offsets\n","    grid = np.arange(grid_size)\n","    a,b = np.meshgrid(grid, grid)\n","\n","    x_offset = torch.FloatTensor(a).view(-1,1)\n","    y_offset = torch.FloatTensor(b).view(-1,1)\n","\n","    if CUDA:\n","        x_offset = x_offset.cuda()\n","        y_offset = y_offset.cuda()\n","\n","    x_y_offset = torch.cat((x_offset, y_offset), 1).repeat(1,num_anchors).view(-1,2).unsqueeze(0)\n","\n","    prediction[:,:,:2] += x_y_offset\n","\n","    #log space transform height and the width\n","    anchors = torch.FloatTensor(anchors)\n","\n","    if CUDA:\n","        anchors = anchors.cuda()\n","\n","    anchors = anchors.repeat(grid_size*grid_size, 1).unsqueeze(0)\n","    prediction[:,:,2:4] = torch.exp(prediction[:,:,2:4])*anchors\n","    \n","    prediction[:,:,5: 5 + num_classes] = torch.sigmoid((prediction[:,:, 5 : 5 + num_classes]))\n","\n","    prediction[:,:,:4] *= stride\n","    \n","    return prediction\n","    "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"I9NTT98Nc61-","colab_type":"code","colab":{}},"source":["class EmptyLayer(nn.Module):\n","    def __init__(self):\n","        super(EmptyLayer, self).__init__()\n","        \n","\n","class DetectionLayer(nn.Module):\n","    def __init__(self, anchors):\n","        super(DetectionLayer, self).__init__()\n","        self.anchors = anchors     \n","                "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"K0rT0qiwc62H","colab_type":"code","colab":{}},"source":["def create_modules(blocks):\n","    net_info = blocks[0]     #Captures the information about the input and pre-processing    \n","    module_list = nn.ModuleList()\n","    prev_filters = 3\n","    output_filters = []\n","    \n","    for index, x in enumerate(blocks[1:]):\n","        module = nn.Sequential()\n","    \n","        #check the type of block\n","        #create a new module for the block\n","        #append to module_list\n","        \n","        #If it's a convolutional layer\n","        if (x[\"type\"] == \"convolutional\"):\n","            #Get the info about the layer\n","            activation = x[\"activation\"]\n","            try:\n","                batch_normalize = int(x[\"batch_normalize\"])\n","                bias = False\n","            except:\n","                batch_normalize = 0\n","                bias = True\n","        \n","            filters= int(x[\"filters\"])\n","            padding = int(x[\"pad\"])\n","            kernel_size = int(x[\"size\"])\n","            stride = int(x[\"stride\"])\n","        \n","            if padding:\n","                pad = (kernel_size - 1) // 2\n","            else:\n","                pad = 0\n","        \n","            #Add the convolutional layer\n","            conv = nn.Conv2d(prev_filters, filters, kernel_size, stride, pad, bias = bias)\n","            module.add_module(\"conv_{0}\".format(index), conv)\n","        \n","            #Add the Batch Norm Layer\n","            if batch_normalize:\n","                bn = nn.BatchNorm2d(filters)\n","                module.add_module(\"batch_norm_{0}\".format(index), bn)\n","        \n","            #Check the activation. \n","            #It is either Linear or a Leaky ReLU for YOLO\n","            if activation == \"leaky\":\n","                activn = nn.LeakyReLU(0.1, inplace = True)\n","                module.add_module(\"leaky_{0}\".format(index), activn)\n","        \n","            #If it's an upsampling layer\n","            #We use Bilinear2dUpsampling\n","        elif (x[\"type\"] == \"upsample\"):\n","            stride = int(x[\"stride\"])\n","            upsample = nn.Upsample(scale_factor = 2, mode = \"nearest\")\n","            module.add_module(\"upsample_{}\".format(index), upsample)\n","                \n","        #If it is a route layer\n","        elif (x[\"type\"] == \"route\"):\n","            x[\"layers\"] = x[\"layers\"].split(',')\n","            #Start  of a route\n","            start = int(x[\"layers\"][0])\n","            #end, if there exists one.\n","            try:\n","                end = int(x[\"layers\"][1])\n","            except:\n","                end = 0\n","            #Positive anotation\n","            if start > 0: \n","                start = start - index\n","            if end > 0:\n","                end = end - index\n","            route = EmptyLayer()\n","            module.add_module(\"route_{0}\".format(index), route)\n","            if end < 0:\n","                filters = output_filters[index + start] + output_filters[index + end]\n","            else:\n","                filters= output_filters[index + start]\n","    \n","        #shortcut corresponds to skip connection\n","        elif x[\"type\"] == \"shortcut\":\n","            shortcut = EmptyLayer()\n","            module.add_module(\"shortcut_{}\".format(index), shortcut)\n","            \n","        #Yolo is the detection layer\n","        elif x[\"type\"] == \"yolo\":\n","            mask = x[\"mask\"].split(\",\")\n","            mask = [int(x) for x in mask]\n","    \n","            anchors = x[\"anchors\"].split(\",\")\n","            anchors = [int(a) for a in anchors]\n","            anchors = [(anchors[i], anchors[i+1]) for i in range(0, len(anchors),2)]\n","            anchors = [anchors[i] for i in mask]\n","    \n","            detection = DetectionLayer(anchors)\n","            module.add_module(\"Detection_{}\".format(index), detection)\n","                              \n","        module_list.append(module)\n","        prev_filters = filters\n","        output_filters.append(filters)\n","    return (net_info, module_list)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlhSfAIZc62O","colab_type":"code","colab":{}},"source":["class Darknet(nn.Module):\n","    def __init__(self, cfgfile):\n","        super(Darknet, self).__init__()\n","        self.blocks = parse_cfg(cfgfile)\n","        self.net_info, self.module_list = create_modules(self.blocks)\n","        \n","    def forward(self, x, CUDA):\n","        modules = self.blocks[1:]\n","        outputs = {}   #We cache the outputs for the route layer\n","        \n","        write = 0\n","        for i, module in enumerate(modules):        \n","            module_type = (module[\"type\"])\n","            \n","            if module_type == \"convolutional\" or module_type == \"upsample\":\n","                x = self.module_list[i](x)\n","    \n","            elif module_type == \"route\":\n","                layers = module[\"layers\"]\n","                layers = [int(a) for a in layers]\n","    \n","                if (layers[0]) > 0:\n","                    layers[0] = layers[0] - i\n","    \n","                if len(layers) == 1:\n","                    x = outputs[i + (layers[0])]\n","    \n","                else:\n","                    if (layers[1]) > 0:\n","                        layers[1] = layers[1] - i\n","    \n","                    map1 = outputs[i + layers[0]]\n","                    map2 = outputs[i + layers[1]]\n","                    x = torch.cat((map1, map2), 1)\n","                \n","    \n","            elif  module_type == \"shortcut\":\n","                from_ = int(module[\"from\"])\n","                x = outputs[i-1] + outputs[i+from_]\n","    \n","            elif module_type == 'yolo':        \n","                anchors = self.module_list[i][0].anchors\n","                #Get the input dimensionsthis video. Please check back late\n","                inp_dim = int (self.net_info[\"height\"])\n","        \n","                #Get the number of classes\n","                num_classes = int (module[\"classes\"])\n","        \n","                #Transform \n","                x = x.data\n","                x = predict_transform(x, inp_dim, anchors, num_classes, CUDA)\n","                if not write:              #if no collector has been intialised. \n","                    detections = x\n","                    write = 1\n","        \n","                else:       \n","                    detections = torch.cat((detections, x), 1)\n","        \n","            outputs[i] = x\n","        \n","        return detections\n","    def load_weights(self , weightfile):\n","        fp = open(weightfile , \"rb\")\n","        header = np.fromfile(fp , dtype = np.int32 , count = 5)\n","        self.header = torch.tensor(header)\n","        self.seen = self.header[3]\n","        weights = np.fromfile(fp , dtype = np.float32)\n","        ptr = 0\n","        for i in range(len(self.module_list)):\n","            module_type = self.blocks[i + 1][\"type\"]\n","            if(module_type == \"convolutional\"):\n","                model = self.module_list[i]\n","                try:\n","                    batch_normalize = int(self.blocks[i + 1][\"batch_normalize\"])\n","                except:\n","                    batch_normalize = 0\n","                conv = model[0]\n","                if (batch_normalize):\n","                    bn = model[1]\n","\n","                    #Get the number of weights of Batch Norm Layer\n","                    num_bn_biases = bn.bias.numel()\n","\n","                    #Load the weights\n","                    bn_biases = torch.from_numpy(weights[ptr:ptr + num_bn_biases])\n","                    ptr += num_bn_biases\n","\n","                    bn_weights = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n","                    ptr  += num_bn_biases\n","\n","                    bn_running_mean = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n","                    ptr  += num_bn_biases\n","\n","                    bn_running_var = torch.from_numpy(weights[ptr: ptr + num_bn_biases])\n","                    ptr  += num_bn_biases\n","\n","                    #Cast the loaded weights into dims of model weights. \n","                    bn_biases = bn_biases.view_as(bn.bias.data)\n","                    bn_weights = bn_weights.view_as(bn.weight.data)\n","                    bn_running_mean = bn_running_mean.view_as(bn.running_mean)\n","                    bn_running_var = bn_running_var.view_as(bn.running_var)\n","\n","                    #Copy the data to model\n","                    bn.bias.data.copy_(bn_biases)\n","                    bn.weight.data.copy_(bn_weights)\n","                    bn.running_mean.copy_(bn_running_mean)\n","                    bn.running_var.copy_(bn_running_var)\n","\n","                else:\n","                    num_biases = conv.bias.numel()\n","\n","                    #Load the weights\n","                    conv_biases = torch.tensor(weights[ptr: ptr + num_biases])\n","                    ptr = ptr + num_biases\n","                    #reshape the loaded weights according to the dims of the model weights\n","                    conv_biases = conv_biases.view_as(conv.bias.data)\n","                    #Finally copy the data\n","                    conv.bias.data.copy_(conv_biases)\n","\n","                #Let us load the weights for the Convolutional layers\n","                num_weights = conv.weight.numel()\n","\n","                #Do the same as above for weights\n","                conv_weights = torch.from_numpy(weights[ptr:ptr+num_weights])\n","                ptr = ptr + num_weights\n","\n","                conv_weights = conv_weights.view_as(conv.weight.data)\n","                conv.weight.data.copy_(conv_weights)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RiYThODlc62W","colab_type":"code","outputId":"6d7f35c6-87c1-4b1b-efc6-e106e7ac163c","executionInfo":{"status":"ok","timestamp":1564340720759,"user_tz":-540,"elapsed":7288,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"loading object detector...\")\n","model = Darknet(yolo_config_path).to(device).eval()\n","model.load_weights(yolo_weights_path)\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["loading object detector...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gBDExI72c62g","colab_type":"code","colab":{}},"source":["def unique(tensor):\n","    tensor_np = tensor.cpu().numpy()\n","    unique_np = np.unique(tensor_np)\n","    unique_tensor = torch.tensor(unique_np)\n","    tensor_res = tensor.new(unique_tensor.shape)\n","    tensor_res.copy_(unique_tensor)\n","    return tensor_res"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OFn1B4Yfc62p","colab_type":"code","colab":{}},"source":["def bbox_iou(box1 , box2):\n","    b1_x1, b1_y1, b1_x2, b1_y2 = box1[:,0], box1[:,1], box1[:,2], box1[:,3]\n","    b2_x1, b2_y1, b2_x2, b2_y2 = box2[:,0], box2[:,1], box2[:,2], box2[:,3]\n","    \n","    inter_rect_x1 =  torch.max(b1_x1, b2_x1)\n","    inter_rect_y1 =  torch.max(b1_y1, b2_y1)\n","    inter_rect_x2 =  torch.min(b1_x2, b2_x2)\n","    inter_rect_y2 =  torch.min(b1_y2, b2_y2)\n","    inter_area = torch.clamp(inter_rect_x2 - inter_rect_x1 + 1, min=0) * torch.clamp(inter_rect_y2 - inter_rect_y1 + 1, min=0)\n","    #Union Area\n","    b1_area = (b1_x2 - b1_x1 + 1)*(b1_y2 - b1_y1 + 1)\n","    b2_area = (b2_x2 - b2_x1 + 1)*(b2_y2 - b2_y1 + 1)\n","    \n","    iou = inter_area / (b1_area + b2_area - inter_area)\n","    return iou"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GBeAE_VYc62z","colab_type":"code","colab":{}},"source":["def write_results(prediction , confidence , num_classes , nms_conf = .4):\n","    conf_mask = (prediction[: , : , 4] > confidence).float().unsqueeze(2)\n","    prediction = prediction*conf_mask\n","    box_corner = prediction.new(prediction.shape)\n","    box_corner[:,:,0] = (prediction[:,:,0] - prediction[:,:,2]/2)\n","    box_corner[:,:,1] = (prediction[:,:,1] - prediction[:,:,3]/2)\n","    box_corner[:,:,2] = (prediction[:,:,0] + prediction[:,:,2]/2) \n","    box_corner[:,:,3] = (prediction[:,:,1] + prediction[:,:,3]/2)\n","    prediction[:,:,:4] = box_corner[:,:,:4]\n","    batch_size = prediction.size(0)\n","    write = False\n","    for ind in range(batch_size):\n","        image_pred = prediction[ind]\n","        max_conf , max_conf_score  = torch.max(image_pred[: , 5:5 + num_classes] , 1)\n","        max_conf = max_conf.float().unsqueeze(1)\n","        max_conf_score = max_conf_score.float().unsqueeze(1)\n","        seq = (image_pred[:,:5] , max_conf , max_conf_score)\n","        image_pred = torch.cat(seq , 1)\n","        non_zero_ind = (torch.nonzero(image_pred[:,4]))\n","        try:\n","            image_pred_ = image_pred[non_zero_ind.squeeze() , :].view(-1,7)\n","        except:\n","            continue\n","        if(image_pred_.shape[0] == 0):\n","            continue\n","        img_classes = unique(image_pred_[:,-1])\n","        for cls in img_classes:\n","            cls_mask = image_pred_*(image_pred_[:,-1] == cls).float().unsqueeze(1)\n","            class_mask_ind = torch.nonzero(cls_mask[:,-2]).squeeze()\n","            image_pred_class = image_pred_[class_mask_ind].view(-1, 7)\n","            \n","            conf_sort_index = torch.sort(image_pred_class[:,4] , descending = True)[1]\n","            image_pred_class  =image_pred_class[conf_sort_index]\n","            idx  =image_pred_class.size(0)\n","            for i in range(idx):\n","                try:\n","                    ious = bbox_iou(image_pred_class[i].unsqueeze(0), image_pred_class[i+1:])\n","                except ValueError:\n","                    break\n","                except IndexError:\n","                    break\n","                iou_mask = (ious < nms_conf).float().unsqueeze(1)\n","                image_pred_class[i+1:] = image_pred_class[i+1:]*iou_mask\n","                non_zero_ind  = torch.nonzero(image_pred_class[: , 4])\n","                image_pred_class = image_pred_class[non_zero_ind].view(-1,7)\n","            batch_ind = image_pred_class.new(image_pred_class.size(0), 1).fill_(ind) \n","            seq = batch_ind, image_pred_class\n","            if not write:\n","                output = torch.cat(seq , 1)\n","                write = True\n","            else:\n","                out = torch.cat(seq,1)\n","                output  = torch.cat((output,out))\n","                \n","    try:\n","        return output\n","    except:\n","        return 0\n","                \n","    \n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzAAWyOOc628","colab_type":"code","colab":{}},"source":["def load_classes(namesfile):\n","    fp = open(namesfile, \"r\")\n","    names = fp.read().split(\"\\n\")[:-1]\n","    return names"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BVPxcFLBc63D","colab_type":"code","outputId":"f8501f55-02a2-4306-ec91-d2390a3d926f","executionInfo":{"status":"ok","timestamp":1564340721406,"user_tz":-540,"elapsed":7834,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"loading object classes....\")\n","num_classes = 80    #For COCO\n","classes = load_classes(coco_names_path)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["loading object classes....\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"11GXImLfc63M","colab_type":"code","outputId":"a7d07f8a-4e50-474e-e96e-5b7b2d49e801","executionInfo":{"status":"ok","timestamp":1564340725231,"user_tz":-540,"elapsed":11627,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"loading feature extractor...\")\n","vgg16 = models.vgg16(pretrained=True)\n","vgg16.classifier = nn.Sequential(*[vgg16.classifier[i] for i in range(4)])\n","for p in vgg16.parameters():\n","    p.requires_grad = False\n","vgg16 = vgg16.to(device).eval()\n","num_frames = 80\n","dim_embedding = 300    "],"execution_count":14,"outputs":[{"output_type":"stream","text":["loading feature extractor...\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BGI99G3uc63V","colab_type":"code","colab":{}},"source":["def preprocess_frame(image, target_height=224, target_width=224):\n","    if len(image.shape) == 2:\n","        image = np.tile(image[:,:,None], 3)\n","    elif len(image.shape) == 4:\n","        image = image[:,:,:,0]\n","\n","    image = skimage.img_as_float(image).astype(np.float32)\n","    height, width, rgb = image.shape\n","    if width == height:\n","        resized_image = cv2.resize(image, (target_height,target_width))\n","\n","    elif height < width:\n","        resized_image = cv2.resize(image, (int(width * float(target_height)/height), target_width))\n","        cropping_length = int((resized_image.shape[1] - target_height) / 2)\n","        resized_image = resized_image[:,cropping_length:resized_image.shape[1] - cropping_length]\n","\n","    else:\n","        resized_image = cv2.resize(image, (target_height, int(height * float(target_width) / width)))\n","        cropping_length = int((resized_image.shape[0] - target_width) / 2)\n","        resized_image = resized_image[cropping_length:resized_image.shape[0] - cropping_length,:]\n","\n","    return cv2.resize(resized_image, (target_height, target_width))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HV93_Gyic63b","colab_type":"code","colab":{}},"source":["def convert_names(name):\n","  if(name == 'traffic light'):\n","    return \"traffic\"\n","  elif(name == \"fire hydrant\"):\n","    return \"extinguisher\"\n","  elif(name == \"stop sign\"):\n","    return \"signboard\"\n","  elif(name == \"parking meter\"):\n","    return \"meter\"\n","  elif(name == \"sports ball\"):\n","    return \"ball\"\n","  elif(name == \"baseball bat\"):\n","    return \"bat\"\n","  elif(name == \"pottedplant\"):\n","    return \"plant\"\n","  elif(name == \"baseball glove\"):\n","    return \"glove\"\n","  elif(name == \"tennis racket\"):\n","    return \"racket\"\n","  elif(name == \"wine glass\"):\n","    return \"glass\"\n","  elif(name == \"hot dog\"):\n","    return \"food\"\n","  elif(name == \"diningtable\"):\n","    return \"table\"\n","  elif(name == \"tvmonitor\"):\n","    return \"tv\"\n","  elif(name ==  'cell phone'):\n","    return \"phone\"\n","  elif(name == \"teddy bear\"):\n","    return \"toy\"\n","  elif(name == \"hair drier\"):\n","    return \"drier\"\n","  elif(name == \"aeroplane\"):\n","    return \"plane\" \n","  else:\n","    return name"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ANEzPz-wc63i","colab_type":"code","outputId":"3da2c119-3528-4d90-a741-28ca1c1badb8","executionInfo":{"status":"ok","timestamp":1564340725252,"user_tz":-540,"elapsed":11611,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"Reading your video...(hope i enjoy it)\")\n","cap = cv2.VideoCapture(video_path)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Reading your video...(hope i enjoy it)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"J0snyZHPc63t","colab_type":"code","outputId":"4679c3be-924e-4ccc-b8e0-a0af56f7c0fa","executionInfo":{"status":"ok","timestamp":1564340734896,"user_tz":-540,"elapsed":21242,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["frame_count = 0\n","frame_list = []\n","\n","while True:\n","    ret , frame = cap.read()   \n","    if(ret == False):\n","        break\n","    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","    frame_list.append(frame)\n","    frame_count = frame_count + 1\n","frame_list = np.array(frame_list)\n","if frame_count > 80:\n","    frame_indices = np.linspace(0, frame_count, num=num_frames, endpoint=False).astype(int)\n","    frame_list = frame_list[frame_indices]\n","cropped_array = []\n","for i in range(frame_list.shape[0]):\n","    cropped_array.append(preprocess_frame(frame_list[i]).transpose(2,0,1))\n","vgg_tensor = torch.tensor(cropped_array).to(device)\n","\n","yolo_tensor  = torch.tensor(cropped_array).to(device)\n","norm = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","for i in range(vgg_tensor.size(0)):\n","    vgg_tensor[i] = norm(vgg_tensor[i])\n","object_list = np.zeros((yolo_tensor.size(0) , dim_embedding))\n","for yolo_frame in range(yolo_tensor.size(0)):\n","    inp = yolo_tensor[yolo_frame:yolo_frame+1]\n","    cuda = False\n","    if(torch.cuda.is_available()):\n","        cuda = True\n","    pred = model(inp , cuda)\n","    res = write_results(pred , .65 ,80 )\n","      \n","    if(type(res) is int):\n","        pass\n","    else:\n","        _, index = torch.sort(res[: , 5] , 0 , descending = True)\n","        res = res[index][0].to(\"cpu\").numpy()\n","        class_index = int(res[-1].item())\n","        obj  = classes[class_index]\n","        object_list[yolo_frame] = embedding[wordtoindex[convert_names(obj)]].to(\"cpu\").numpy() \n","vgg_features  = vgg16(vgg_tensor).detach().to(\"cpu\").numpy()\n","print(\"yaaaaawn .... That was a boring video :3\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["yaaaaawn .... That was a boring video :3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bY-HTMVNc631","colab_type":"code","outputId":"4b8a2414-ede0-4cc0-f3f3-5ddadf5bbcc0","executionInfo":{"status":"ok","timestamp":1564340734897,"user_tz":-540,"elapsed":21231,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["object_features = torch.tensor(object_list).to(device).double().unsqueeze(0)\n","vgg_features = torch.tensor(vgg_features).to(device).double().unsqueeze(0)\n","print(\"Loading caption model\")"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Loading caption model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"rHuPR1ej57LR","colab_type":"code","colab":{}},"source":["class Video_captioner(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.embedding = nn.Embedding(len(wordtoindex), embedding_dim)\n","        self.embedding.weight = nn.Parameter(embedding)\n","        self.embedding.weight.requires_grad = False\n","        self.lstm1 = nn.LSTM(dim_input1 , dim_hidden1, batch_first = True)\n","        self.lstm2 = nn.LSTM(dim_input2 , dim_hidden2 , batch_first = True)\n","        self.img_encode = nn.Linear(dim_image , dim_input1)\n","        self.embed_word = nn.Linear(dim_hidden2 ,len(wordtoindex))\n","    def forward(self  , video , objects , feed , teacher = True ):\n","        emb = self.embedding(feed)\n","        out  = self.img_encode(video)\n","        #encoding\n","        out11 , state11 = self.lstm1(out)\n","        lstm2_inp = torch.cat( (objects , out11) , dim = 2   )\n","        out12 , state12 = self.lstm2(lstm2_inp)\n","        #decoding\n","        if(teacher == True):\n","            padding = torch.zeros(video.size(0) , feed.size(1) , dim_input1).to(device).double()\n","            out21 , state21 = self.lstm1(padding, state11)\n","            lstm2_inp2 = torch.cat( (emb , out21)  , dim = 2 )\n","            out22 , state22 = self.lstm2(lstm2_inp2 , state12)\n","        else:\n","            padding = torch.zeros(video.size(0) , max_caption_length-1 , dim_input1).to(device).double()\n","            out21 , state21 = self.lstm1(padding , state11)\n","            out22 = torch.zeros(video.size(0) , max_caption_length  -1, dim_hidden2).to(device).double()\n","            state__ = state12\n","            for i in range(max_caption_length -1):\n","                inp_lstm2 = torch.cat((emb , out21[: , i:i+1, :]) , dim = 2)\n","                out__ , state__ = self.lstm2(inp_lstm2 , state__)\n","                out22[: , i:i+1 , :] = out__\n","                with torch.no_grad():\n","                    out__  = self.embed_word(out__)\n","                    out__ = torch.argmax(out__ , dim = 2)                    \n","                    out__ = self.embedding(out__)\n","                    emb = out__\n","                    \n","                    \n","                \n","                \n","        out22 = out22.contiguous()    \n","        soft  = self.embed_word(out22)\n","        meaning_out = torch.matmul(soft , embedding)\n","        return soft.view(-1, len(wordtoindex)) , meaning_out \n","        \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fgn1zS-SeUyh","colab_type":"code","colab":{}},"source":["class Attention_model(nn.Module):\n","    def __init__(self ):\n","        super().__init__()\n","        self.embedding = nn.Embedding(len(wordtoindex), embedding_dim)\n","        self.embedding.weight = nn.Parameter(embedding)\n","        self.embedding.weight.requires_grad = False\n","        self.img_encode = nn.Linear(dim_image , dim_input1)\n","        self.lstm1 = nn.LSTM(dim_input1 , dim_hidden1, batch_first = True)\n","        self.lstm2 = nn.LSTM(dim_input2 , dim_hidden2 , batch_first = True)\n","        self.embed_word = nn.Linear(dim_hidden2 ,len(wordtoindex))\n","        self.attn = nn.Linear(dim_hidden1  , dim_hidden2)\n","        self.tanh = nn.Tanh()\n","        self.context_inp = nn.Linear(dim_hidden1 , dim_hidden1)\n","    def calculate_attention(self, prev_out , encoder_out):\n","        seq_len = encoder_out.size(1)\n","        encoder_out = self.attn(encoder_out)\n","        encoder_out = encoder_out.transpose(1,2)\n","        att_energy = torch.bmm(prev_out , encoder_out)\n","        att_energy =  F.softmax(att_energy , dim = 2)\n","        return att_energy\n","    \n","    def decoder_teacher(self , encoder_out , emb , decoder_state):\n","        prev_out = torch.zeros(encoder_out.size(0) , 1 , dim_hidden2).to(device).double()\n","        out22 = torch.zeros( encoder_out.size(0), max_caption_length -1 , dim_hidden2).to(device).double()\n","        for i in range(max_caption_length - 1):\n","            attention_energy = self.calculate_attention(prev_out , encoder_out)\n","            context = torch.bmm(attention_energy , encoder_out)\n","            context = self.context_inp(context)\n","            lstm2_inp2 = torch.cat((emb[: , i:i+1 , :] , context ), dim = 2)\n","            decoder_out , decoder_state = self.lstm2(lstm2_inp2 , decoder_state)\n","            out22[: , i:i+1 , :] = decoder_out\n","            prev_out = decoder_out\n","        return out22\n","    def decoder_non_teacher(self , encoder_out , emb , decoder_state):\n","        prev_out = torch.zeros(encoder_out.size(0) , 1 , dim_hidden2).to(device).double()\n","        out22 = torch.zeros(encoder_out.size(0) , max_caption_length -1 , dim_hidden2).to(device).double()\n","        for i in range(max_caption_length - 1):\n","            attention_energy = self.calculate_attention(prev_out , encoder_out)\n","            context = torch.bmm(attention_energy , encoder_out)\n","            context = self.context_inp(context)\n","            lstm2_inp2 = torch.cat((emb , context ), dim = 2)\n","            decoder_out , decoder_state = self.lstm2(lstm2_inp2 , decoder_state)\n","            out22[: , i:i+1 , :] = decoder_out\n","            prev_out = decoder_out\n","            with torch.no_grad():\n","                decoder_out  = self.embed_word(decoder_out)\n","                decoder_out = torch.argmax(decoder_out , dim = 2)                    \n","                emb = self.embedding(decoder_out)\n","        return out22\n","        \n","            \n","        \n","        \n","\n","    def forward(self  , video , objects , feed , teacher = True):\n","        emb = self.embedding(feed)\n","        video  = self.img_encode(video)\n","        # encode\n","        out11  , state11 = self.lstm1(video)\n","        lstm2_inp = torch.cat( (objects , out11) , dim = 2 )\n","        out12  , decoder_state  = self.lstm2(lstm2_inp)\n","        encoder_out = out11\n","        # Decode\n","        if(teacher == True):\n","            out22  = self.decoder_teacher(encoder_out , emb , decoder_state)\n","        else:\n","            out22 = self.decoder_non_teacher(encoder_out , emb , decoder_state)\n","            \n","        out22 = out22.contiguous() \n","        soft  = self.embed_word(out22)\n","        meaning_out = torch.matmul(soft , embedding)\n","        return soft.view(-1, len(wordtoindex)) , meaning_out "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BxHD75UwiEIJ","colab_type":"code","colab":{}},"source":["video_model = Attention_model().to(device).double().eval()\n","#video_model =  Video_captioner().to(device).double().eval()\n","bos = torch.tensor(wordtoindex[\"<bos>\"]).to(device)\n","bos = bos.unsqueeze(0).unsqueeze(0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NtPXSUuwjAYE","colab_type":"code","colab":{}},"source":["def generate(model_num):\n","    if(torch.cuda.is_available()):\n","        video_model.load_state_dict(torch.load(os.path.join(model_save_path , \"model\" + str(model_num) + \".pt\")  ))\n","    else:\n","        video_model.load_state_dict(torch.load(os.path.join(model_save_path , \"model\" + str(model_num) + \".pt\") ,map_location='cpu'   ))\n","    sent = []\n","    video , yolo = vgg_features , object_features\n","    out_ = video_model(video , yolo , bos , False)\n","    out_ = torch.argmax(out_[0] , dim = 1).to(\"cpu\").numpy()\n","    for i in out_:\n","        w = indextoword[i]\n","        if(w == \"<eos>\"):\n","            break\n","        sent.append(w)\n","    sent = \" \".join(sent)\n","\n","    print(\" \")\n","    print(\" \")\n","    print(\">> \" , sent , \" <<\")\n","        \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EQ4v716Sk_Rk","colab_type":"code","outputId":"b8e0e809-2f5c-40be-aea5-1d857261f303","executionInfo":{"status":"ok","timestamp":1564340737400,"user_tz":-540,"elapsed":23662,"user":{"displayName":"RUSHI JAYESHKUMAR BABARIYA","photoUrl":"","userId":"09653925580240235591"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["print(\"The caption is......drum rollllll\")\n","generate(model_num)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["The caption is......drum rollllll\n"," \n"," \n",">>  a woman is washing a box  <<\n"],"name":"stdout"}]}]}